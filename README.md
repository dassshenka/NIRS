# Q-обучение для строительства пирамиды

Этот проект реализует алгоритм Q-обучения для обучения агента строить устойчивую пирамиду из блоков в физической среде. Агент учится оптимально размещать блоки, чтобы максимизировать награду и избежать падения конструкции.

## Особенности проекта

- Реализация Q-обучения с ε-жадной стратегией
- Физическое моделирование с помощью Pymunk
- Визуализация процесса обучения с помощью Pygame
- Статистика обучения и график наград
- Сохранение и загрузка Q-таблицы

## Зависимости

Для работы проекта необходимы следующие библиотеки:

    pip install pygame pymunk numpy matplotlib
## Структура проекта

├── main.py              # Основной игровой цикл и визуализация

├── physics.py           # Настройка физического пространства

├── utils.py             # Утилиты для создания блоков

├── agent.py             # Реализация Q-агента

├── config.py            # Конфигурационные параметры

├── game.py              # Логика игрового процесса

└── README.md            # Документация (этот файл)
## Установка и запуск
Клонируйте репозиторий::

    git clone https://github.com/dassshenka/NIRS.git
    cd NIRS
Установите зависимости:

    pip install -r requirements.txt
Запустите приложение:

    python main.py
> Примечание: первое окно может показаться «пустым» — агенту нужно время, чтобы исследовать действия. Через \~100 поколений начнёт проявляться структура.
## Управление и интерфейс

- Программа запускает процесс обучения автоматически
- Каждые SHOW_EVERY поколений отображается визуализация
- В левом верхнем углу отображается статистика:
  - Gen: номер текущего поколения
  - Blocks: количество размещенных блоков
  - Best: лучший результат
  - Epsilon: текущий коэффициент исследования
- При закрытии окна сохраняется Q-таблица и отображается график наград

## Настройка параметров

Основные параметры можно изменить в файле config.py:

    # Разрешение окна
      WIDTH, HEIGHT = 900, 900

    # Частота обновления кадров
      FPS = 150

    # Размер блока
      BLOCK_SIZE = 125

    # Начальная позиция блока
      START_X = WIDTH // 2
      START_Y = 30

    # Максимальное количество блоков
      MAX_BLOCKS = 30

    # Частота отображения процесса
      SHOW_EVERY = 100

  Частоту смены кадров регулируем в файле main.py:

    # Контроль частоты кадров
    clock.tick(10000) 

## Алгоритм работы

1. Агент получает текущее состояние (высоты столбцов)
2. На основе Q-таблицы и ε-стратегии выбирает действие (позицию для блока)
3. Блок создается в выбранной позиции
4. Симулируется физика падения блока
5. Вычисляется награда на основе устойчивости конструкции
6. Q-таблица обновляется по формуле Q-обучения
7. Процесс повторяется до завершения эпизода

## Сохранение прогресса

Q-таблица автоматически сохраняется в файл q_table.pkl при завершении программы. При следующем запуске происходит автоматическая загрузка предыдущего состояния.

## График обучения

После завершения программы отображается график наград по поколениям, позволяющий оценить прогресс обучения.

![image](https://github.com/user-attachments/assets/84c741d7-1bd3-478e-b9c6-c0f730b52677)

